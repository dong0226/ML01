{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-PTReiOw-RAN"
   },
   "outputs": [],
   "source": [
    "# Adapted from https://huggingface.co/learn/deep-rl-course/unit6/advantage-actor-critic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Unit 6: Advantage Actor Critic (A2C) using Robotics Simulations with Panda-Gym ü§ñ\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit8/thumbnail.png\"  alt=\"Thumbnail\"/>\n",
    "\n",
    "In this notebook, you'll learn to use A2C with [Panda-Gym](https://github.com/qgallouedec/panda-gym). You're going **to train a robotic arm** (Franka Emika Panda robot) to perform a task:\n",
    "\n",
    "- `Reach`: the robot must place its end-effector at a target position.\n",
    "\n",
    "After that, you'll be able **to train in other robotics tasks**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bTpYcVZVMzUI"
   },
   "source": [
    "## Create a virtual display üîΩ\n",
    "\n",
    "During the notebook, we'll need to generate a replay video. To do so, with colab, **we need to have a virtual screen to be able to render the environment** (and thus record the frames).\n",
    "\n",
    "Hence the following cell will install the librairies and create and run a virtual screen üñ•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ww5PQH1gNLI4"
   },
   "outputs": [],
   "source": [
    "# Virtual display\n",
    "from pyvirtualdisplay import Display\n",
    "\n",
    "virtual_display = Display(visible=0, size=(1400, 900))\n",
    "virtual_display.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QTep3PQQABLr"
   },
   "source": [
    "## Import the packages üì¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HpiB8VdnQ7Bk"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gymnasium\n",
    "import panda_gym\n",
    "\n",
    "import sys\n",
    "sys.modules[\"gym\"] = gymnasium\n",
    "import gymnasium as gym\n",
    "\n",
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
    "from stable_baselines3.common.env_util import make_vec_env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lfBwIS_oAVXI"
   },
   "source": [
    "## PandaReachDense-v3 ü¶æ\n",
    "\n",
    "The agent we're going to train is a robotic arm that needs to do controls (moving the arm and using the end-effector).\n",
    "\n",
    "In robotics, the *end-effector* is the device at the end of a robotic arm designed to interact with the environment.\n",
    "\n",
    "In `PandaReach`, the robot must place its end-effector at a target position (green ball).\n",
    "\n",
    "We're going to use the dense version of this environment. It means we'll get a *dense reward function* that **will provide a reward at each timestep** (the closer the agent is to completing the task, the higher the reward). Contrary to a *sparse reward function* where the environment **return a reward if and only if the task is completed**.\n",
    "\n",
    "Also, we're going to use the *End-effector displacement control*, it means the **action corresponds to the displacement of the end-effector**. We don't control the individual motion of each joint (joint control).\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit8/robotics.jpg\"  alt=\"Robotics\"/>\n",
    "\n",
    "\n",
    "This way **the training will be easier**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "frVXOrnlBerQ"
   },
   "source": [
    "## Create the environment\n",
    "\n",
    "#### The environment üéÆ\n",
    "\n",
    "In `PandaReachDense-v3` the robotic arm must place its end-effector at a target position (green ball)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zXzAu3HYF1WD"
   },
   "outputs": [],
   "source": [
    "env_id = \"PandaReachDense-v3\"\n",
    "\n",
    "# Create the env\n",
    "env = gym.make(env_id)\n",
    "\n",
    "# Get the state space and action space\n",
    "s_size = env.observation_space.shape\n",
    "a_size = env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E-U9dexcF-FB"
   },
   "outputs": [],
   "source": [
    "print(\"_____OBSERVATION SPACE_____ \\n\")\n",
    "print(\"The State Space is: \", s_size)\n",
    "print(\"Sample observation\", env.observation_space.sample()) # Get a random observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g_JClfElGFnF"
   },
   "source": [
    "The observation space **is a dictionary with 3 different elements**:\n",
    "- `achieved_goal`: (x,y,z) position of the goal.\n",
    "- `desired_goal`: (x,y,z) distance between the goal position and the current object position.\n",
    "- `observation`: position (x,y,z) and velocity of the end-effector (vx, vy, vz).\n",
    "\n",
    "Given it's a dictionary as observation, **we will need to use a MultiInputPolicy policy instead of MlpPolicy**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ib1Kxy4AF-FC"
   },
   "outputs": [],
   "source": [
    "print(\"\\n _____ACTION SPACE_____ \\n\")\n",
    "print(\"The Action Space is: \", a_size)\n",
    "print(\"Action Space Sample\", env.action_space.sample()) # Take a random action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# env = make_vec_env(env_id, n_envs=4)\n",
    "env = make_vec_env(env_id, n_envs=50)\n",
    "\n",
    "env = VecNormalize(env, norm_obs=True, norm_reward=True, clip_obs=10.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4JmEVU6z1ZA-"
   },
   "source": [
    "## Create the A2C Model ü§ñ\n",
    "\n",
    "For more information about A2C implementation with StableBaselines3 check: https://stable-baselines3.readthedocs.io/en/master/modules/a2c.html#notes\n",
    "\n",
    "To find the best parameters I checked the [official trained agents by Stable-Baselines3 team](https://huggingface.co/sb3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FKFLY54T-pU1"
   },
   "outputs": [],
   "source": [
    "model = A2C(policy = \"MultiInputPolicy\",\n",
    "            env = env,\n",
    "            device=\"cpu\",\n",
    "            verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "opyK3mpJ1-m9"
   },
   "source": [
    "## Train the A2C agent üèÉ\n",
    "- Let's train our agent for 1,000,000 timesteps, don't forget to use GPU on Colab. It will take approximately ~25-40min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4TuGHZD7RF1G",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.learn(1_000_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MfYtjj19cKFr"
   },
   "outputs": [],
   "source": [
    "# Save the model and  VecNormalize statistics when saving the agent\n",
    "model.save(\"a2c-PandaReachDense-v3\")\n",
    "env.save(\"vec_normalize.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "01M9GCd32Ig-"
   },
   "source": [
    "## Evaluate the agent üìà\n",
    "- Now that's our  agent is trained, we need to **check its performance**.\n",
    "- Stable-Baselines3 provides a method to do that: `evaluate_policy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "liirTVoDkHq3"
   },
   "outputs": [],
   "source": [
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
    "\n",
    "# Load the saved statistics\n",
    "eval_env = DummyVecEnv([lambda: gym.make(\"PandaReachDense-v3\")])\n",
    "eval_env = VecNormalize.load(\"vec_normalize.pkl\", eval_env)\n",
    "\n",
    "# We need to override the render_mode\n",
    "eval_env.render_mode = \"rgb_array\"\n",
    "\n",
    "#  do not update them at test time\n",
    "eval_env.training = False\n",
    "# reward normalization is not needed at test time\n",
    "eval_env.norm_reward = False\n",
    "\n",
    "# Load the agent\n",
    "model = A2C.load(\"a2c-PandaReachDense-v3\")\n",
    "\n",
    "mean_reward, std_reward = evaluate_policy(model, eval_env, render = True)\n",
    "\n",
    "print(f\"Mean reward = {mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Record Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "from pathlib import Path\n",
    "\n",
    "from IPython import display as ipythondisplay\n",
    "\n",
    "def show_videos(video_path='', prefix=''):\n",
    "  \"\"\"\n",
    "  Taken from https://github.com/eleurent/highway-env\n",
    "\n",
    "  :param video_path: (str) Path to the folder containing videos\n",
    "  :param prefix: (str) Filter the video, showing only the only starting with this prefix\n",
    "  \"\"\"\n",
    "  html = []\n",
    "  for mp4 in Path(video_path).glob(\"{}*.mp4\".format(prefix)):\n",
    "      video_b64 = base64.b64encode(mp4.read_bytes())\n",
    "      html.append('''<video alt=\"{}\" autoplay \n",
    "                    loop controls style=\"height: 400px;\">\n",
    "                    <source src=\"data:video/mp4;base64,{}\" type=\"video/mp4\" />\n",
    "                </video>'''.format(mp4, video_b64.decode('ascii')))\n",
    "  ipythondisplay.display(ipythondisplay.HTML(data=\"<br>\".join(html)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.vec_env import VecVideoRecorder, DummyVecEnv\n",
    "\n",
    "def record_video(env_id, model, video_length=500, prefix='', video_folder='videos/'):\n",
    "  \"\"\"\n",
    "  :param env_id: (str)\n",
    "  :param model: (RL model)\n",
    "  :param video_length: (int)\n",
    "  :param prefix: (str)\n",
    "  :param video_folder: (str)\n",
    "  \"\"\"\n",
    "\n",
    "  eval_env = DummyVecEnv([lambda: gym.make(\"PandaReachDense-v3\")])\n",
    "  eval_env = VecNormalize.load(\"vec_normalize.pkl\", eval_env)\n",
    "\n",
    "  # We need to override the render_mode\n",
    "  eval_env.render_mode = \"rgb_array\"\n",
    "\n",
    "  #  do not update them at test time\n",
    "  eval_env.training = False\n",
    "  # reward normalization is not needed at test time\n",
    "  eval_env.norm_reward = False\n",
    "\n",
    "  # Start the video at step=0 and record 500 steps\n",
    "  eval_env = VecVideoRecorder(eval_env, video_folder=video_folder,\n",
    "                              record_video_trigger=lambda step: step == 0, video_length=video_length,\n",
    "                              name_prefix=prefix)\n",
    "\n",
    "  obs = eval_env.reset()\n",
    "  for _ in range(video_length):\n",
    "    action, _ = model.predict(obs)\n",
    "    obs, _, _, _ = eval_env.step(action)\n",
    "\n",
    "  # Close the video recorder\n",
    "  eval_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_video('PandaReachDense-v3', model, video_length=250, prefix='a2c-PandaReachDense-v3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_videos('videos', prefix='a2c-PandaReachDense-v3')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "tF42HvI7-gs5"
   ],
   "private_outputs": true,
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
